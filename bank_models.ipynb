{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# to handle the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to visualize the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# to preprocess the data\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder    \n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# sampling\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# model\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import matthews_corrcoef as MCC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.metrics import f1_score as f1\n",
    "\n",
    "# adjust parameters\n",
    "import optuna\n",
    "from pyswarm import pso\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# adjust parameters\n",
    "import optuna\n",
    "\n",
    "# max columns \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from collections import Counter\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ed0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "df = pd.read_csv(\"bank-full.csv\",sep = ';')\n",
    "df = shuffle(df,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30656f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_binary(df_train,df_test):\n",
    "    # 'default','housing','loan' - binary\n",
    "    # no = 0, yes =1\n",
    "    label_encoder = LabelEncoder()\n",
    "    object_cols = ['default','housing','loan','y']\n",
    "    for col in object_cols:\n",
    "        df_train[col] = label_encoder.fit_transform(df_train[col])\n",
    "        df_test[col] = label_encoder.transform(df_test[col])\n",
    "    return df_train,df_test\n",
    "\n",
    "def onehot(df):\n",
    "    cat_cols = ['marital','education','contact','poutcome','month','job']\n",
    "    #onehotEncoding\n",
    "    try:\n",
    "        df=pd.get_dummies(df,columns=cat_cols)\n",
    "        return df\n",
    "    except:\n",
    "        print('there is no cat_cols in the df')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706de24",
   "metadata": {},
   "source": [
    "----\n",
    "## Data process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ffad3",
   "metadata": {},
   "source": [
    "Code for data exploration is in bank_EDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72db3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kos_folds = []\n",
    "y_kos_folds = []\n",
    "df_test_folds = []\n",
    "feat_cols_folds = []\n",
    "k = 5\n",
    "for i in range(k):\n",
    "    \n",
    "    # 5 folds split\n",
    "    n = df.shape[0]//5 # split point\n",
    "    df_train = pd.concat([df[:i*n],df[(i+1)*n:]])\n",
    "    df_test = df[i*n:(i+1)*n]\n",
    "\n",
    "    # process numerical variables\n",
    "    numeirc_cols = ['age','balance','duration','campaign','pdays','previous','day']\n",
    "    for col in numeirc_cols:\n",
    "        sc = MinMaxScaler()\n",
    "        df_train[col+\"_scaled\"] = sc.fit_transform(df_train[[col]])\n",
    "        df_test[col+\"_scaled\"] = sc.transform(df_test[[col]])\n",
    "    \n",
    "    # process categorical variables\n",
    "    df_train,df_test = label_binary(df_train,df_test)\n",
    "    df_train = onehot(df_train)\n",
    "    df_test = onehot(df_test)\n",
    "    \n",
    "    # Selecting Columns For use \n",
    "    feat_cols=df_train.columns.drop(['y'])\n",
    "    feat_cols=feat_cols.drop(numeirc_cols)\n",
    "    \n",
    "    # choose predictor and responser\n",
    "    X=df_train[feat_cols]\n",
    "    y=df_train['y']\n",
    "    \n",
    "    # combining sampling\n",
    "    kos = SMOTETomek(random_state=0)\n",
    "    X_kos, y_kos = kos.fit_resample(X, y)\n",
    "    print('y_kos class:{}'.format(Counter(y_kos)))\n",
    "    print(f'The Shape Of X is {X_kos.shape}')\n",
    "    print(f'The Shape Of y is {y_kos.shape}')\n",
    "    \n",
    "    X_kos_folds.append(X_kos)\n",
    "    y_kos_folds.append(y_kos)\n",
    "    df_test_folds.append(df_test)\n",
    "    feat_cols_folds.append(feat_cols)\n",
    "    \n",
    "print('Finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736b869",
   "metadata": {},
   "source": [
    "---\n",
    "## First iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "nb_mcc = []\n",
    "nb_cm = []\n",
    "nb_accuracy = []\n",
    "nb_precision = []\n",
    "nb_recall = []\n",
    "nb_f1 = []\n",
    "\n",
    "for i in range(k):\n",
    "    X_kos = X_kos_folds[i]\n",
    "    y_kos = y_kos_folds[i]\n",
    "    df_test = df_test_folds[i]\n",
    "    feat_cols = feat_cols_folds[i]\n",
    "    \n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_kos,y_kos)\n",
    "    \n",
    "    test_labels = nb_model.predict(df_test[feat_cols])\n",
    "    cm = confusion_matrix(df_test['y'], test_labels)\n",
    "    mcc = MCC(df_test['y'], test_labels)\n",
    "    nb_cm.append(cm)\n",
    "    nb_mcc.append(mcc)\n",
    "    print(cm,mcc)\n",
    "\n",
    "    nb_accuracy.append(accuracy(df_test['y'], test_labels))\n",
    "    nb_precision.append(precision(df_test['y'], test_labels))\n",
    "    nb_recall.append(recall(df_test['y'], test_labels))\n",
    "    nb_f1.append(f1(df_test['y'], test_labels))\n",
    "\n",
    "print(\"Mean MCC:\", sum(nb_mcc)/5)\n",
    "print(\"Mean accuracy:\", sum(nb_accuracy)/5)\n",
    "print(\"Mean precision:\", sum(nb_precision)/5)\n",
    "print(\"Mean recall:\", sum(nb_recall)/5)\n",
    "print(\"Mean f1:\", sum(nb_f1)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "tr_mcc = []\n",
    "tr_cm = []\n",
    "tr_accuracy = []\n",
    "tr_precision = []\n",
    "tr_recall = []\n",
    "tr_f1 = []\n",
    "\n",
    "for i in range(k):\n",
    "    X_kos = X_kos_folds[i]\n",
    "    y_kos = y_kos_folds[i]\n",
    "    df_test = df_test_folds[i]\n",
    "    feat_cols = feat_cols_folds[i]\n",
    "    \n",
    "    tr_model = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "    tr_model.fit(X_kos,y_kos)\n",
    "    \n",
    "    test_labels = tr_model.predict(df_test[feat_cols])\n",
    "    cm = confusion_matrix(df_test['y'], test_labels)\n",
    "    mcc = MCC(df_test['y'], test_labels)\n",
    "    tr_cm.append(cm)\n",
    "    tr_mcc.append(mcc)\n",
    "    print(cm,mcc)\n",
    "    \n",
    "    tr_accuracy.append(accuracy(df_test['y'], test_labels))\n",
    "    tr_precision.append(precision(df_test['y'], test_labels))\n",
    "    tr_recall.append(recall(df_test['y'], test_labels))\n",
    "    tr_f1.append(f1(df_test['y'], test_labels))\n",
    "\n",
    "print(\"Mean MCC:\", sum(tr_mcc)/5)\n",
    "print(\"Mean accuracy:\", sum(tr_accuracy)/5)\n",
    "print(\"Mean precision:\", sum(tr_precision)/5)\n",
    "print(\"Mean recall:\", sum(tr_recall)/5)\n",
    "print(\"Mean f1:\", sum(tr_f1)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm_mcc = []\n",
    "svm_cm = []\n",
    "svm_accuracy = []\n",
    "svm_precision = []\n",
    "svm_recall = []\n",
    "svm_f1 = []\n",
    "\n",
    "for i in range(k):\n",
    "    X_kos = X_kos_folds[i]\n",
    "    y_kos = y_kos_folds[i]\n",
    "    df_test = df_test_folds[i]\n",
    "    feat_cols = feat_cols_folds[i]\n",
    "    \n",
    "    # train the model\n",
    "    svm_model = svm.SVC(kernel='rbf')\n",
    "    svm_model.fit(X_kos,y_kos)\n",
    "    \n",
    "    test_labels = svm_model.predict(df_test[feat_cols])\n",
    "    cm = confusion_matrix(df_test['y'], test_labels)\n",
    "    mcc = MCC(df_test['y'], test_labels)\n",
    "    svm_cm.append(cm)\n",
    "    svm_mcc.append(mcc)\n",
    "    print(cm,mcc)\n",
    "    \n",
    "    svm_accuracy.append(accuracy(df_test['y'], test_labels))\n",
    "    svm_precision.append(precision(df_test['y'], test_labels))\n",
    "    svm_recall.append(recall(df_test['y'], test_labels))\n",
    "    svm_f1.append(f1(df_test['y'], test_labels))\n",
    "\n",
    "print(\"Mean MCC:\", sum(svm_mcc)/5)\n",
    "print(\"Mean accuracy:\", sum(svm_accuracy)/5)\n",
    "print(\"Mean precision:\", sum(svm_precision)/5)\n",
    "print(\"Mean recall:\", sum(svm_recall)/5)\n",
    "print(\"Mean f1:\", sum(svm_f1)/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b432d628",
   "metadata": {},
   "source": [
    "---\n",
    "## Second iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4659455",
   "metadata": {},
   "source": [
    "Tunning code by optuna and pso is in bank_tunning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': 414,\n",
    "    'max_features': 14,\n",
    "         }\n",
    "\n",
    "rf_mcc = []\n",
    "rf_cm = []\n",
    "rf_accuracy = []\n",
    "rf_precision = []\n",
    "rf_recall = []\n",
    "rf_auc = []\n",
    "rf_f1 = []\n",
    "\n",
    "for i in range(k):\n",
    "    X_kos = X_kos_folds[i]\n",
    "    y_kos = y_kos_folds[i]\n",
    "    df_test = df_test_folds[i]\n",
    "    feat_cols = feat_cols_folds[i]\n",
    "    \n",
    "    # train the model\n",
    "    rf_model = RandomForestClassifier(**rf_params)\n",
    "    rf_model.fit(X_kos,y_kos)\n",
    "    \n",
    "    test_predictions = rf_model.predict_proba(df_test[feat_cols])[:, 1]\n",
    "    test_labels = [i >= 0.5 for i in test_predictions]\n",
    "    \n",
    "    cm = confusion_matrix(df_test['y'], test_labels)\n",
    "    mcc = MCC(df_test['y'], test_labels)\n",
    "    rf_cm.append(cm)\n",
    "    rf_mcc.append(mcc)\n",
    "    print(cm,mcc)\n",
    "    \n",
    "    rf_accuracy.append(accuracy(df_test['y'], test_labels))\n",
    "    rf_precision.append(precision(df_test['y'], test_labels))\n",
    "    rf_recall.append(recall(df_test['y'], test_labels))\n",
    "    rf_auc.append(auc(df_test['y'], test_predictions))\n",
    "    rf_f1.append(f1(df_test['y'], test_labels))\n",
    "    \n",
    "print(\"Mean MCC:\", sum(rf_mcc)/5)\n",
    "print(\"Mean accuracy:\", sum(rf_accuracy)/5)\n",
    "print(\"Mean precision:\", sum(rf_precision)/5)\n",
    "print(\"Mean recall:\", sum(rf_recall)/5)\n",
    "print(\"Mean roc_auc:\", sum(rf_auc)/5)\n",
    "print(\"Mean f1:\", sum(rf_f1)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lgb_params = {\n",
    "        'n_estimators': 1100,\n",
    "        'num_leaves': 104,\n",
    "        'max_depth': 23, \n",
    "        'learning_rate': 0.005,\n",
    "        'min_child_weight': 1e-06,\n",
    "        'min_child_samples': 19, \n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 1, \n",
    "        'verbose':-1\n",
    "            }\n",
    "\n",
    "lgb_mcc = []\n",
    "lgb_cm = []\n",
    "lgb_accuracy = []\n",
    "lgb_precision = []\n",
    "lgb_recall = []\n",
    "lgb_auc = []\n",
    "lgb_f1 = []\n",
    "\n",
    "for i in range(k):\n",
    "    X_kos = X_kos_folds[i]\n",
    "    y_kos = y_kos_folds[i]\n",
    "    df_test = df_test_folds[i]\n",
    "    feat_cols = feat_cols_folds[i]\n",
    "    \n",
    "    # train the model\n",
    "    lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    lgb_model.fit(X_kos,y_kos)\n",
    "    \n",
    "    test_predictions = lgb_model.predict_proba(df_test[feat_cols])[:, 1]\n",
    "    test_labels = [i >= 0.5 for i in test_predictions]\n",
    "    \n",
    "    cm = confusion_matrix(df_test['y'], test_labels)\n",
    "    mcc = MCC(df_test['y'], test_labels)\n",
    "    lgb_cm.append(cm)\n",
    "    lgb_mcc.append(mcc)\n",
    "    print(cm,mcc)\n",
    "    \n",
    "    lgb_accuracy.append(accuracy(df_test['y'], test_labels))\n",
    "    lgb_precision.append(precision(df_test['y'], test_labels))\n",
    "    lgb_recall.append(recall(df_test['y'], test_labels))\n",
    "    lgb_auc.append(auc(df_test['y'], test_predictions))\n",
    "    lgb_f1.append(f1(df_test['y'], test_labels))\n",
    "\n",
    "print(\"Mean MCC:\", sum(lgb_mcc)/5)\n",
    "print(\"Mean accuracy:\", sum(lgb_accuracy)/5)\n",
    "print(\"Mean precision:\", sum(lgb_precision)/5)\n",
    "print(\"Mean recall:\", sum(lgb_recall)/5)\n",
    "print(\"Mean roc_auc:\", sum(lgb_auc)/5)\n",
    "print(\"Mean f1:\", sum(lgb_f1)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8656e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CatBoostClassifier\n",
    "cat_params = {\n",
    "    'eval_metric':'AUC',\n",
    "    'iterations':1000,\n",
    "    'learning_rate':0.025,\n",
    "    'depth':6,\n",
    "    'rsm':0.11,\n",
    "    'subsample':1,\n",
    "    'verbose':False\n",
    "}\n",
    "\n",
    "cat_mcc = []\n",
    "cat_cm = []\n",
    "cat_accuracy = []\n",
    "cat_precision = []\n",
    "cat_recall = []\n",
    "cat_auc = []\n",
    "cat_f1 = []\n",
    "\n",
    "for i in range(k):\n",
    "    X_kos = X_kos_folds[i]\n",
    "    y_kos = y_kos_folds[i]\n",
    "    df_test = df_test_folds[i]\n",
    "    feat_cols = feat_cols_folds[i]\n",
    "    \n",
    "    #Cat_features\n",
    "    cat_features = np.where(X_kos.dtypes != np.float64)[0]\n",
    "\n",
    "    # Train the model on the entire dataset\n",
    "    cat_model = CatBoostClassifier(**cat_params)\n",
    "    train_pool = Pool(X_kos, y_kos, cat_features=cat_features)\n",
    "    cat_model.fit(train_pool)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_pool = Pool(df_test[feat_cols], cat_features=cat_features)\n",
    "    test_predictions = cat_model.predict_proba(test_pool)[:, 1]\n",
    "    test_labels = [i >= 0.5 for i in test_predictions]\n",
    "    \n",
    "    cm = confusion_matrix(df_test['y'], test_labels)\n",
    "    mcc = MCC(df_test['y'], test_labels)\n",
    "    cat_cm.append(cm)\n",
    "    cat_mcc.append(mcc)\n",
    "    print(cm,mcc)\n",
    "    \n",
    "    cat_accuracy.append(accuracy(df_test['y'], test_labels))\n",
    "    cat_precision.append(precision(df_test['y'], test_labels))\n",
    "    cat_recall.append(recall(df_test['y'], test_labels))\n",
    "    cat_auc.append(auc(df_test['y'], test_predictions))\n",
    "    cat_f1.append(f1(df_test['y'], test_labels))\n",
    "\n",
    "\n",
    "print(\"Mean MCC:\", sum(cat_mcc)/5)\n",
    "print(\"Mean accuracy:\", sum(cat_accuracy)/5)\n",
    "print(\"Mean precision:\", sum(cat_precision)/5)\n",
    "print(\"Mean recall:\", sum(cat_recall)/5)\n",
    "print(\"Mean roc_auc:\", sum(cat_auc)/5)\n",
    "print(\"Mean f1:\", sum(cat_f1)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4672e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Parameters\n",
    "xgb_params = {\n",
    "    'n_estimators': 736,\n",
    "    'num_leaves': 994, \n",
    "    'max_depth': 11,\n",
    "    'learning_rate': 0.01,\n",
    "    'min_child_weight': 0.0001,\n",
    "    'min_child_sampes': 23,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc'\n",
    "}\n",
    "xgb_mcc = []\n",
    "xgb_cm = []\n",
    "xgb_accuracy = []\n",
    "xgb_precision = []\n",
    "xgb_recall = []\n",
    "xgb_auc = []\n",
    "xgb_f1 = []\n",
    "\n",
    "for i in range(k):\n",
    "    X_kos = X_kos_folds[i]\n",
    "    y_kos = y_kos_folds[i]\n",
    "    df_test = df_test_folds[i]\n",
    "    feat_cols = feat_cols_folds[i]\n",
    "    \n",
    "    # train the model\n",
    "    xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "    xgb_model.fit(X_kos,y_kos)\n",
    "    \n",
    "    test_predictions = xgb_model.predict_proba(df_test[feat_cols])[:, 1]\n",
    "    test_labels = [i >= 0.5 for i in test_predictions]\n",
    "    \n",
    "    cm = confusion_matrix(df_test['y'], test_labels)\n",
    "    mcc = MCC(df_test['y'], test_labels)\n",
    "    xgb_cm.append(cm)\n",
    "    xgb_mcc.append(mcc)\n",
    "    print(cm,mcc)\n",
    "    \n",
    "    xgb_accuracy.append(accuracy(df_test['y'], test_labels))\n",
    "    xgb_precision.append(precision(df_test['y'], test_labels))\n",
    "    xgb_recall.append(recall(df_test['y'], test_labels))\n",
    "    xgb_auc.append(auc(df_test['y'], test_predictions))\n",
    "    xgb_f1.append(f1(df_test['y'], test_labels))\n",
    "\n",
    "print(\"Mean MCC:\", sum(xgb_mcc)/5)\n",
    "print(\"Mean accuracy:\", sum(xgb_accuracy)/5)\n",
    "print(\"Mean precision:\", sum(xgb_precision)/5)\n",
    "print(\"Mean recall:\", sum(xgb_recall)/5)\n",
    "print(\"Mean roc_auc:\", sum(xgb_auc)/5)\n",
    "print(\"Mean f1:\", sum(xgb_f1)/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87101f5f",
   "metadata": {},
   "source": [
    "---\n",
    "## Third iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad1438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoostClassifier with PSO\n",
    "cat_params = {\n",
    "    'eval_metric':'AUC',\n",
    "    'iterations':1000,\n",
    "    'learning_rate':0.0098,\n",
    "    'depth':9,\n",
    "    'rsm':0.1287,\n",
    "    'subsample':0.8294,\n",
    "    'verbose':False\n",
    "}\n",
    "\n",
    "catP_mcc = []\n",
    "catP_cm = []\n",
    "catP_accuracy = []\n",
    "catP_precision = []\n",
    "catP_recall = []\n",
    "catP_auc = []\n",
    "catP_f1 = []\n",
    "\n",
    "for i in range(k):\n",
    "    X_kos = X_kos_folds[i]\n",
    "    y_kos = y_kos_folds[i]\n",
    "    df_test = df_test_folds[i]\n",
    "    feat_cols = feat_cols_folds[i]\n",
    "    \n",
    "    #Cat_features\n",
    "    catP_features = np.where(X_kos.dtypes != np.float64)[0]\n",
    "\n",
    "    # Train the model on the entire dataset\n",
    "    catP_model = CatBoostClassifier(**cat_params)\n",
    "    train_pool = Pool(X_kos, y_kos, cat_features=catP_features)\n",
    "    catP_model.fit(train_pool)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_pool = Pool(df_test[feat_cols], cat_features=catP_features)\n",
    "    test_predictions = catP_model.predict_proba(test_pool)[:, 1]\n",
    "    test_labels = [i >= 0.5 for i in test_predictions]\n",
    "    \n",
    "    cm = confusion_matrix(df_test['y'], test_labels)\n",
    "    mcc = MCC(df_test['y'], test_labels)\n",
    "    catP_cm.append(cm)\n",
    "    catP_mcc.append(mcc)\n",
    "    print(cm,mcc)\n",
    "    \n",
    "    catP_accuracy.append(accuracy(df_test['y'], test_labels))\n",
    "    catP_precision.append(precision(df_test['y'], test_labels))\n",
    "    catP_recall.append(recall(df_test['y'], test_labels))\n",
    "    catP_auc.append(auc(df_test['y'], test_predictions))\n",
    "    catP_f1.append(f1(df_test['y'], test_labels))\n",
    "\n",
    "print(\"Mean MCC:\", sum(catP_mcc)/5)\n",
    "print(\"Mean accuracy:\", sum(catP_accuracy)/5)\n",
    "print(\"Mean precision:\", sum(catP_precision)/5)\n",
    "print(\"Mean recall:\", sum(catP_recall)/5)\n",
    "print(\"Mean roc_auc:\", sum(catP_auc)/5)\n",
    "print(\"Mean f1:\", sum(catP_f1)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c770d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for the plot\n",
    "classifiers = ['CatBoost with PSO','LightGBM', 'CatBoost','XGBoost','RandomForest','SVM','NaiveBayes','DecisionTree']\n",
    "mcc_scores = [sum(catP_mcc)/5,sum(lgb_mcc)/5,sum(cat_mcc)/5,sum(xgb_mcc)/5,sum(rf_mcc)/5,sum(svm_mcc)/5,sum(nb_mcc)/5, sum(tr_mcc)/5]\n",
    "\n",
    "# Create the figure with optimized settings\n",
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=classifiers,\n",
    "    y=[i for i in mcc_scores],\n",
    "    name='Accuracy',\n",
    ")])\n",
    "\n",
    "# Update layout with optimized settings\n",
    "fig.update_layout(\n",
    "    title='MCC Comparison',\n",
    "    xaxis_title='Classifier',\n",
    "    yaxis_title='MCC Score',\n",
    "    template='plotly_white',\n",
    "    font=dict(family=\"Arial\", size=12),\n",
    "    width=600,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)\n",
    ")\n",
    "\n",
    "# Add gridlines\n",
    "fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='#E0E0E0')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b566894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
